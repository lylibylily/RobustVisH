
# ğŸ­ RobustVisH: Robust Visual-Haptic Cross-Modal Recognition Under Transmission Interference (ACM MM 2025)

![RobustVisH](hello.png)

**Status**: Supplementary materials for the manuscript *"RobustVisH: RobustVisual-Haptic Cross-Modal Recognition Under Transmission Interference"*.

## ğŸ“ Repository Structure
```bash
.
â”œâ”€â”€ README.md          # You are here
â”œâ”€â”€ requirements-RobustVisH.txt   # Python dependencies
â”œâ”€â”€ requirements-WITIM.txt     # Python dependencies for WITIM benchmark
â”œâ”€â”€ hello.png          # Model overview image
â”œâ”€ RobustVisH/        # RobustVisH model implementation
â”‚  â”œâ”€â”€ lib/               # Core implementation
â”‚  â”‚    â”œâ”€â”€ models/          # Model architectures
â”‚  â”‚    â”‚    â””â”€â”€ cnnBiGRUbisa.py        # Main model definition
â”‚  â”‚    â””â”€â”€ data/              # Data processing scripts & sample dataset
â”‚  â”‚         â”œâ”€â”€ RegNet_Y_32GF.py
â”‚  â”‚         â””â”€â”€ DataLoader.py              # Dataset loader
â”‚  â”œâ”€â”€ weights/       # Pre-trained models
â”‚  â”‚    â”œâ”€â”€ RobustVisH-AU.h5          # Pre-trained on Action Unit dataset
â”‚  â”‚    â””â”€â”€ RobustVisH-PHAC-2.h5      # Pre-trained on PHAC-2 dataset
â”‚  â”œâ”€â”€ clr_callback.py            # Learning rate scheduler
â”‚  â”œâ”€â”€ model_test.py              # Evaluation pipeline
â”‚  â””â”€â”€ model_train.py             # Training pipeline
â””â”€ WITIM/            # WIreless Transmission Interference-based Multi-modal benchmark
    â”œâ”€â”€ gmsk_haptic.grc
    â”œâ”€â”€ gmsk_haptic.py
    â”œâ”€â”€ gmsk_visual.grc
    â”œâ”€â”€ gmsk_visual.py
    â”œâ”€â”€ haptic_batch_run.bat
    â”œâ”€â”€ visual_batch_run.bat
    â””â”€â”€ WITIM.png
```

## ğŸš€ Quick Start
1. Environment Setup
```bash
# Create conda environment (recommended)
conda create -n RobustVisH python=3.8
conda activate RobustVisH
# Install dependencies
pip install -r requirements for RobustVisH.txt
```

2. Data Preparation
```bash
# 1. Download dataset and use WITIM
# 2. Preprocess data
python RobustVisH/lib/data/RegNet_Y_32GF.py
```
3. Model Training
```bash
python RobustVisH/model_train.py
```
4. Inference
```bash
python RobustVisH/model_test.py
```

## ğŸ”® Pre-trained Models
| Dataset | Accuracy | F1-score | Model Checkpoint |
|---------|----------|----------|------------------|
| AU      | 91.11%   | 0.9061   | RobustVisH/weights/RobustVisH-AU.h5 |
| PHAC-2  | 61.81%   | 0.6210   | RobustVisH/weights/RobustVisH-PHAC-2.h5 |

## ğŸ“œ Citation
If you find this work useful, please cite our preprint:
```bibtex
@inproceedings{RobustVisH2025,
  title={RobustVisH: Robust Visual-Haptic Cross-Modal Recognition Under Transmission Interference},
  author={Zhang, Rouqi and Lu, Chengdi and Lu, Hancheng and Cao, Yang and Zhao, Tiesong},
  booktitle={Proceedings of the 33rd ACM International Conference on Multimedia (MM'25)},
  year={2025},
  url={https://openreview.net/forum?id=v1v7zRmL3o}
}
```

## âš ï¸ Important Notes
**Hardware Requirements**: Recommended NVIDIA RTX2080Ti or better GPU for training.
